% !BIB program = biber
% !BIB TS-program = biber

\documentclass[11pt, oneside]{report}

% set up some more document options
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage[english]{babel}

\usepackage{geometry}
\geometry{letterpaper, margin = 1in, bottom = 1in}

% various packages
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{relsize}
\usepackage{microtype}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{endnotes}
\usepackage{textcomp}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{lipsum}
\usepackage[table]{xcolor}
\usepackage{listingsutf8,multicol}
\usepackage{chngcntr}
\usepackage[justification=centering]{caption}
\usepackage{comment}

% font for listings
\usepackage[T1]{fontenc}
\usepackage[scaled]{beramono}
\newcommand\Small{\fontsize{8}{8.15}\selectfont}
\newcommand\Tiny{\fontsize{6}{6.1}\selectfont}
\newcommand*\LSTFont{\Small\ttfamily\SetTracking{encoding=*}{-60}\lsstyle}
\newcommand*\LSTLineNumberFont{\Tiny\ttfamily\SetTracking{encoding=*}{-60}\lsstyle}

% configure listings (for source code excerpts)
\definecolor{listinggray}{gray}{0.9}
\definecolor{listingbackground}{rgb}{0.9,0.9,0.9}
\definecolor{listingscol}{rgb}{0.6,0.6,0.6}
\lstset{
	backgroundcolor=\color{listingbackground},
	tabsize=4,
	rulecolor=\color{listingscol},
	language=c,
        basicstyle=\LSTFont,
        upquote=true,
		numbers=left,							% Line nums position
		numberstyle=\LSTLineNumberFont,			% Line-numbers fonts
		stepnumber=1,                           % Step between two line-numbers
		numbersep=5pt,                          % How far are line-numbers from code
        aboveskip={1.5\baselineskip},
        showstringspaces=true,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=shadowbox,
		breaklines=true,                        % Automatic line breaking?
        showtabs=true,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
}

% a command to allow text to justify and word-wrap
\newcommand*\justify{%
	\fontdimen2\font=0.4em% interword space
	\fontdimen3\font=0.2em% interword stretch
	\fontdimen4\font=0.1em% interword shrink
	\fontdimen7\font=0.1em% extra space
	\hyphenchar\font=`\-% allowing hyphenation
}

% hide the "Chapter N" text and set spacing	
\makeatletter
\def\@makechapterhead#1{%
	\vspace*{40\p@}
	{\parindent \z@ \raggedright \normalfont
		\ifnum \c@secnumdepth >\m@ne
			\par\nobreak
			\vskip 20\p@
		\fi
		\interlinepenalty\@M
		\huge \bfseries #1\par\nobreak
		\vskip 20\p@
	}
}
\makeatother

% set up glossary
\usepackage[toc, nonumberlist]{glossaries}
\makeglossaries

\include{include/glossary/terms}
\include{include/glossary/acronyms}

% bibliography
\usepackage[sorting=nty,backend=biber,style=verbose-trad1]{biblatex}

\addbibresource{include/bibliography.bib}

% begin document
\begin{document}

% title page
\begin{titlepage}
	\centering
	{\scshape\LARGE Westwood High School \par}
	\vspace{0.66cm}
	{\scshape\Large IB Extended Essay\par}
	\vspace{1.5cm}
	{\huge\bfseries What are some common 3D rendering techniques, and their impact on the visual quality and performance, that can be used to achieve photorealism?\par}
	\vspace{1.5cm}
	{\Large\itshape Tristan Seifert\par}
	\vspace{0.33cm}
	{\large Word Count: 2511\par}
	\fcolorbox{black}{red}{\Large\itshape {\color{white} \uppercase{Draft Copy}}}
	\vfill
	
	IB Candidate Number:\par
	000883-0108

	\vfill

	{\large October 27, 2015 \par}
\end{titlepage}

% table of contents
\tableofcontents

% introduction to CGI and the problems with photorealism
\chapter{Introduction}
\section{A Short History of 3D Graphics}
Ever since computers have been able to output graphics in any form, people have always sought to get better quality out of them. Compared to the blocky, low-resolution graphics of the 1970s, today’s incredibly realistic 3D graphics have come incredibly far. However, this incredible realism comes at a cost: developers must balance precisely the graphical quality they desire with the capabilities of available hardware.

Many techniques have been developed—most in the last decade alone—that simplify natural phenomena so that computers can efficiently approximate them for graphics. The requirements of each project are different, but this paper will look at several techniques commonly to most 3D visualizations (such as videogames,) and analyze what effect they have on the performance of the rendering pipeline, and compare the effects they have on the realism and appearance of the output.

\section{Real-time 3D Graphics}
While computers have been able to produce incredibly photorealistic 3D scenes for decades, it has only been recently that the computing capacity of \glspl{GPU} has caught up and made it possible to do so in realtime: a prime example of which are modern videogames.

To be considered realtime, graphics must be rendered at interactive \glspl{frame rate}: rates at which the human eye is unable to discern the individual frames, and they flow together into one smooth image: similarly to how a film is many different frames shown in rapid succession.

Typically, 60 \gls{fps} is used, as most displays' \glspl{refresh rate} are 60Hz---though 30\gls{fps} and 120\gls{fps} have been growing in popularity recently.

However, despite these advances in computing capacity, achieving both photorealism and interactive \glspl{frame rate}---the 'forbidden fruit' of 3D graphics---still remains an incredibly difficult task: a task which requires many clever techniques and trade-offs to 'fake' the way in which objects interact in a 3D space.

\subsection{Limiting Factors}
When rendering a scene, an immense amount of information needs to be rapidly accessed by the \gls{GPU} to actually produce an output, much of which is heavily processed by \glspl{shader} before it is even displayed. The speed at which this information can be read from memory, known as the \gls{memory bandwidth} is often a limiting factor: a problem known as memory bus saturation. Other times---especially on lower-end \glspl{GPU}---the \glspl{compute unit} are fully utilized and cannot operate any faster, causing the rest of the \gls{graphics pipeline} to stall: an unsurprising fact, considering that many effects rely on incredibly complex vector mathematics.

\section{Analyzing Improvements}
To analyze the effects of various techniques—\gls{deferred shading}, \gls{HDR}, \gls{bloom}, \gls{shadow mapping}, \gls{ambient occlusion}, and \gls{FXAA}—I developed a testbed with a flexible rendering pipeline that has features similar to those of a modern video game. It is written entirely in standards-compliant \texttt{C99} and \texttt{C++1y}.

When coupled with debuggers, static and dynamic analyzers, such as Apple's Instruments and \gls{OpenGL} Profiler, very accurate data about the impacts of these techniques on performance can be acquired, down to a breakdown of which line of code takes the longest to execute.

To maintain a constant environment, all tests will be run on the same machine: a mid-2012 MacBook Pro, featuring a 2.3GHz Intel i7, 16GB of RAM, and an NVIDIA GeForce GT 650M, with 512MB of video memory. Additionally, all tests are ran in windowed mode, at a resolution of $1024 \times 768$ pixels, using \gls{OpenGL} version 4.2.

% where each of the techniques is analyzed 
\chapter{Deferred Shading}
Traditionally, rendering pipelines calculated lighting information for every \gls{texel}, whether it would be displayed or not, wasting an immense amount of resources, even if invisible geometry is discarded early through \glspl{depth test}.

Deferred rendering performs complex lighting calculations \textit{after} all geometry has been rendered, freeing up significant resources for other tasks\footcite{gpupro-deferred}. More realistic lighting can be implemented: for example, by using a higher exponent (thus leading to smoother reflections) with the \gls{Blinn-Phong reflection model}.\footcite{ferko-deferred}

To achieve all of this, deferred shading works by rendering all texels into a \gls{G Buffer}, then running an additional shading pass to perform lighting calculations all at once.

\section{Geometry Buffer}
The geometry buffer is a collection of distinct buffers: \gls{specular} highlights (colours of sampled textures) and albedo; surface normals (used in calculating reflections) and fragment depth. Position is required for lighting calculations, but can be derived from depth via some transformations.

\begin{figure}[!htbp]
  \centering
  \subfloat[Albedo and \gls{specular}]{\includegraphics[width=0.32\textwidth]{images/lighting_gBuf_albedoSpec.png}\label{fig:f1}}
  \hfill
  \subfloat[Surface normals]{\includegraphics[width=0.32\textwidth]{images/lighting_gBuf_normals.png}\label{fig:f2}}
  \hfill
  \subfloat[Depth]{\includegraphics[width=0.32\textwidth]{images/lighting_gBuf_depth.png}\label{fig:f3}}
  \caption{Components of the G buffer in the testbed's deferred shading implementation.}
\end{figure}

Rendering of objects in the scene is performed by a simple \gls{shader}, which serves to consolidate its inputs, mixing them as appropriate, and writing them to the G buffer. While this does require a significant amount of additional video memory---particularly since the normal and depth buffers need high precision \glspl{float} to represent their values---it simplifies the lighting processing.

Information about lights, encoded as structures, is sent to a shader, which also takes the G buffer as an input. It performs the necessary calculations for each \gls{texel} and outputs it to the next stage in the rendering pipeline.

\section{Lighting Calculations}
Thanks to the flexibility of performing all lighting at once, many different types of lighting can be implemented. In the testbed, four types of lighting are supported: ambient light, directional lights, point lights, and spotlights. Each of these lights has an associated \gls{specular} and \gls{diffuse} colour.

\subsection{Ambient Light}
Ambient light is an average of all non-specific light sources in a scene, with a fixed colour, affecting every \gls{texel} equally.

\subsection{Directional Light}
Directional lights are an approximation of light sources that are infinitely far away, modeled as a series of parallel light rays. They have a diffuse and specular colour, and a direction that indicates which way the light rays will be cast. Any \gls{texel} that intersects with a light ray from a directional light will be affected by it.

Directional lights are the only lights that will cause shadows to be cast.

\subsection{Point Light}
Point lights are similar to directional lights, except they have a position instead of a direction. Light rays are cast in all directions from this center point, and like directional lights, any \gls{texel} in their path will be affected by it. Instead of a constant effect, however, point lights' influence gets weaker the farther the \gls{texel} is from the light source.

This attenuation is defined by a constant ($K_c$), a linear ($K_l$) and a quadratic ($K_q$) term, as well as the distance from the light ($d$) and unattenuated intensity ($I$): \begin{equation} F_{att} = \frac{I}{K_c + K_l * d + K_q * d^2} \end{equation}

To get the effect a point light has on a \gls{texel}, its specular and diffuse colours are multiplied by $F_{att}$.

\subsection{Spotlight}
Spotlights are special cases of point lights, with a direction in addition to a position, as well as a radius. They cast light rays as a cone with a given radius, and illuminate everything within. Toward the edge of the circle, the intensity of the light begins to rapidly fall off.

\section{End Result}
When combining all of these lighting types, a good approximation of the real environment can be created.

\begin{figure}[!htbp]
   \centering
   \includegraphics[width=0.74\textwidth]{images/lighting_out.png}
   \caption{Output of the lighting stage, without gamma compensation, with values > \texttt{1.0} clipped. A \gls{skybox} was rendered in areas where lighting calculations produced no output.}
   \label{fig:example}
\end{figure}

But what about particularly bright lights, or really dark ones? The standard approach to rendering them will cause \glspl{texel} illuminated by them to appear as solid bright or dark areas, losing most detail. \gls{HDR} solves that problem, by having the lighting pass output an intermediate, unbounded representation of light intensity.

A downside of deferred shading is that objects that previously affected light in unique ways are more difficult to implement, since all lighting calculations are in the same shader. Additionally, a significant amount of memory is consumed by the \gls{G Buffer}, so it is not uncommon to reuse components of the \gls{G Buffer} for other purposes.

% add some timing information here pls!

\chapter{High Dynamic Range (HDR) and Bloom}
One of the biggest problems of computer graphics has traditionally been to approach the dynamic range of the human eye, even though display devices have a limited colour palette they can display. Particularly, the human eye is much better at recovering detail from very bright and very dark areas than a computer display can show, so a large range of brightness values must somehow be mapped onto the handful of nonlinear brightness values displayed by computer monitors: this is exactly what \gls{HDR} does.

Additionally, due to optical imperfections in lenses (the eye is really one big lens) there often appears bleeding of light from very bright to darker areas. By applying \gls{bloom}, the brightness of a light source can be exaggerated and shown more clearly.

\section{Producing HDR Output}
The deferred shading pass outputs colour values into a \gls{floating point} buffer, allowing for a nearly infinite amount of brightness values to be expressed. The conversion between HDR values and \gls{RGB} values is relatively straightforward, and is defined by a function in a \gls{shader}; also known as \gls{tone mapping}. These values are also adjusted to match a certain \gls{white point}\footcite{hdr}.

\begin{figure}[!htbp]
  \centering
  \subfloat[Before]{\includegraphics[width=0.49\textwidth]{images/lighting_out.png}\label{fig:f1}}
  \hfill
  \subfloat[After]{\includegraphics[width=0.49\textwidth]{images/bloom_hdr_out.png}\label{fig:f2}}
  \caption{Before and after \gls{tone mapping}, gamma and white point adjustments.}
\end{figure}

In addition to a particular \gls{tone mapping} algorithm, the sensitivity of \gls{HDR} can easily be adjusted via an exposure parameter. This parameter serves as a constant multiplier for the \gls{HDR} input colours before \gls{tone mapping}, and affects the overall brightness of the image: similarly to how changing the exposure settings on a photographic camera will affect the brightness of the photograph.

\begin{figure}[!htbp]
  \centering
  \subfloat[-3]{\includegraphics[width=0.19\textwidth]{images/exposure/EV_-3.jpg}\label{fig:f1}}
  \hfill
  \subfloat[-1.5]{\includegraphics[width=0.19\textwidth]{images/exposure/EV_-1_5.jpg}\label{fig:f2}}
  \hfill
  \subfloat[0]{\includegraphics[width=0.19\textwidth]{images/exposure/EV_0.jpg}\label{fig:f3}}
  \hfill
  \subfloat[1.5]{\includegraphics[width=0.19\textwidth]{images/exposure/EV_1_5.jpg}\label{fig:f4}}
  \hfill
  \subfloat[3]{\includegraphics[width=0.19\textwidth]{images/exposure/EV_3.jpg}\label{fig:f5}}
  \caption{Effects of varying exposure values (EV) on a photograph}
\end{figure}

Overall, HDR can produce a great improvement in visual quality with little additional work. An extra stage of shader processing before output adds minimal overhead, and no additional memory is needed, if a buffer from a previous stage in the rendering pipeline is reused.

Various additional effects can be achieved simply by varying the exposure value: for example, a higher exposure value could be used for night-time scenes, and a lower one for day-time scenes. Perhaps even automatic exposure adjustment, where the overall brightness is analyzed, and exposure is slowly changed to maintain a baseline level of brightness, similar to how the human eye functions.

\section{Bloom}
Blooming simulates the glow that occurs around extremely bright light sources. In conjunction with \gls{HDR}, it is incredibly easy to implement, with little additional overhead.

All fragments that are considered bright---a combined brightness of \texttt{1.0} or above---are copied into an additional buffer. This buffer is half the size as the output screen, saving memory and processing time. A second buffer of identical size is allocated. This buffer is then blurred through several iterations of a Gaussian blur until an adequate blur has been achieved.

\begin{figure}[!htbp]
  \centering
  \subfloat[Before]{\includegraphics[width=0.49\textwidth]{images/bloom_bright_in.png}\label{fig:f1}}
  \hfill
  \subfloat[After]{\includegraphics[width=0.49\textwidth]{images/bloom_pass4.png}\label{fig:f2}}
  \caption{Bright input fragments, before and after application of the Gaussian blur.}
\end{figure}

The Gaussian blur itself consists of a $13 \times 13$ blur kernel, which is approximated by sampling the texture seven times for each \gls{texel}, using bilinear interpolation to get more samples. The blur itself has been decomposed into separate horizontal and vertical components for performance reasons, and is run over a set of two buffers---the original 'bright fragment' input buffer, and the eventual 'output' buffer---a predetermined number of times.

Once blurring has been completed, the output buffer is sampled in the \gls{HDR} output shader, multiplied by a coefficient (this determines the strength and effect of the blur on the rest of the scene,) then added to the \gls{HDR} colour.

\section{End Result}
By combining both \gls{HDR} and blooming, the lighting in a scene already appears much more realistic, solving the issue of washed out highlights and details that disappear into the shadows. These two render passes require little in the way of additional memory, and their \gls{shader} programs are deceptively simple. Additionally, the \gls{HDR} pass serves as a place for gamma correction to take place. Textures are stored as sRGB, and \gls{OpenGL}'s built-in conversion is disabled. This way, the user can configure the gamma of the application to match their monitor most closely, instead of relying on a hard-coded value in a graphics driver.

When analyzing the testbed's performance with \gls{HDR} and blooming enabled, the little additional overhead incurred by the technique immediately becomes clear. On average, only 713µS of processing time and 1.7MB of video memory were needed to perform all associated calculations, once code had been optimized to minimize \glspl{pipeline stall}.

\chapter{Fast Approximate Antialiasing (FXAA)}
Due to the limited resolution of textures and buffers, as well as the multitude of transformations applied to geometric primitives, it is extremely common for an unprocessed output to exhibit heavy \gls{aliasing}. Most commonly, it will take the form of jagged edges.

In the past, aliasing was combatted by rendering the entire scene at a much higher resolution---often 2x or 4x larger than the screen---then simply downscaling it, creating a primitive form of supersampling. Later on, similar techniques were applied to \glspl{shader}, when they sampled textures, improving performance somewhat; also known as MSAA. Supersampling works in a similar manner.

What all of these antialiasing algorithms have in common is that they are very computationally expensive. They can double or quadruple the rendering time, while yielding a minimal benefit.

\section{Implementation}
Processing the output with \gls{FXAA} is relatively straightforward. A buffer is created, into which the final output of all previous stages of the rendering pipeline is stored, instead of going to the window framebuffer.

This 'colour buffer' is then given as an input to the \gls{FXAA} \gls{shader} program, which samples it systematically, detects edges, smoothes them, and outputs a final antialiased output to the window framebuffer\footcite{nvidia-fxaa}.

\begin{figure}[!htbp]
  \centering
  \subfloat[With FXAA]{\includegraphics[width=0.49\textwidth]{images/fxaa_out@8x.png}\label{fig:f1}}
  \hfill
  \subfloat[Without FXAA]{\includegraphics[width=0.49\textwidth]{images/bloom_hdr_out@8x.png}\label{fig:f2}}
  \caption{A crop from the final output, with and without FXAA. Note the rough edges on on (b).}
\end{figure}

Because \gls{FXAA} is implemented in a shader\footnote{Example implementation from NVIDIA, Version 3.11 by Timothy Lottes}, rather than in hardware, its behavior (such as edge detection sensitivity, smoothing algorithm and sharpness, etc.) can be adjusted on-the-fly. 

\section{End Result}
By utilizing a new algorithm to approximate antialiasing instead of wasting precious computational resources and \gls{memory bandwidth} on traditional algorithms, immense performance gains can be made. In most cases, the quality of \gls{FXAA} is comparable to that of more traditional algorithms: and most of the time, the precise nature of the antialiasing algorithm makes little difference to the user of the program.

Implementing \gls{FXAA} improves the quality of the output significantly, by removing \gls{aliasing} artifacts, with little impact on the performance of the rendering pipeline. On average, running the \gls{FXAA} pass, using the highest 'low dither' preset, an additional 1.13mS of processing time and 4MB of video memory were needed.

\chapter{Bibliography}

\begingroup
	\def\chapter*#1{}
	\printbibliography[
		heading=bibintoc,
		title={Bibliography}
	]
\endgroup

Trebilco, Damian. \textit{Light Indexed Deferred Lighting.} Tech. N.p., 17 December 2007. Web. 12 Aug. 2015.

\printglossaries

\end{document}